{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Face Cropping, Live Subtitles and pop-up Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing necessary libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* `moviepy.editor` was removed from the current version of `moviepy`. Therefore, we are using `moviepy==1.0.3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoXPoX6lIwQC",
        "outputId": "b76c2241-d13c-49b6-e8b0-6d8b2bb9ce4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: webrtcvad in /usr/local/lib/python3.11/dist-packages (2.0.10)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install numpy opencv-python moviepy==1.0.3 pydub webrtcvad torch torchaudio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Smooth Face Tracking and Cropping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiAcrIt0DPrN",
        "outputId": "86c3310e-becc-4060-c682-911d4c78da09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Writing audio in temp.wav\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Building video face_tracked_output.mp4.\n",
            "MoviePy - Writing audio in face_tracked_outputTEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video face_tracked_output.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "t: 100%|██████████| 2126/2126 [01:39<00:00, 28.67it/s, now=None]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_video.mp4, 2764800 bytes wanted but 0 bytes read,at frame 2125/2126, at time 88.58/88.59 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready face_tracked_output.mp4\n",
            "Video processing complete!\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip\n",
        "from pydub import AudioSegment\n",
        "import webrtcvad\n",
        "import os\n",
        "from collections import deque\n",
        "\n",
        "def extract_audio(video_path, audio_output=\"temp.wav\"):\n",
        "    video = VideoFileClip(video_path)\n",
        "    video.audio.write_audiofile(audio_output, codec=\"pcm_s16le\")\n",
        "    return audio_output, video.audio\n",
        "\n",
        "def detect_speech(audio_path, aggressiveness=3):\n",
        "    audio = AudioSegment.from_file(audio_path, format=\"wav\")\n",
        "    audio = audio.set_frame_rate(16000).set_channels(1)\n",
        "    vad = webrtcvad.Vad(aggressiveness)\n",
        "\n",
        "    speech_intervals = []\n",
        "    window_duration = 0.03\n",
        "    frame_rate = audio.frame_rate\n",
        "    samples_per_window = int(frame_rate * window_duration)\n",
        "\n",
        "    for i in range(0, len(audio.raw_data), samples_per_window * 2):\n",
        "        chunk = audio.raw_data[i:i + samples_per_window * 2]\n",
        "        if len(chunk) < samples_per_window * 2:\n",
        "            continue\n",
        "        is_speech = vad.is_speech(chunk, frame_rate)\n",
        "        timestamp = i / (frame_rate * 2)\n",
        "        if is_speech:\n",
        "            speech_intervals.append((timestamp, timestamp + window_duration))\n",
        "\n",
        "    return speech_intervals\n",
        "\n",
        "def load_face_detector():\n",
        "    net = cv2.dnn.readNetFromCaffe(\n",
        "        \"deploy.prototxt\",\n",
        "        \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "    )\n",
        "    return net\n",
        "\n",
        "def detect_faces(frame, net):\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "    faces = []\n",
        "    for i in range(detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "        if confidence > 0.5:\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            x1, y1, x2, y2 = box.astype(\"int\")\n",
        "            x1, y1, x2, y2 = max(0, x1), max(0, y1), min(x2, w), min(y2, h)\n",
        "            faces.append((x1, y1, x2 - x1, y2 - y1))\n",
        "    return faces\n",
        "\n",
        "def moving_average(values, window_size=5):\n",
        "    if not values:\n",
        "        return None\n",
        "    smoothed = np.mean(values, axis=0).astype(int)\n",
        "    return tuple(smoothed)\n",
        "\n",
        "def process_video(input_path, output_path, speech_intervals, detection_skip=5, smoothing_window=5):\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    output_width = 720\n",
        "    output_height = 1280\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(\"temp_video.mp4\", fourcc, fps, (output_width, output_height))\n",
        "\n",
        "    net = load_face_detector()\n",
        "    frame_number = 0\n",
        "    last_detected_faces = []\n",
        "    bbox_history = deque(maxlen=smoothing_window)\n",
        "    last_speaker_bbox = None\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        current_time = frame_number / fps\n",
        "        in_speech = any(start <= current_time <= end for (start, end) in speech_intervals)\n",
        "\n",
        "        if in_speech:\n",
        "            if frame_number % detection_skip == 0 or not last_detected_faces:\n",
        "                last_detected_faces = detect_faces(frame, net)\n",
        "\n",
        "            if len(last_detected_faces) == 0:\n",
        "                smoothed_bbox = last_speaker_bbox\n",
        "            else:\n",
        "                if len(last_detected_faces) == 1:\n",
        "                    x, y, w, h = last_detected_faces[0]\n",
        "\n",
        "                    expand_factor = 3.2\n",
        "                    x_new = max(0, int(x - (w * (expand_factor - 1) / 2)))\n",
        "                    w_new = min(width - x_new, int(w * expand_factor))\n",
        "                    y_new = max(0, int(y - (h * (expand_factor - 1) / 2)))\n",
        "                    h_new = min(height - y_new, int(h * expand_factor))\n",
        "\n",
        "                    bbox_history.append((x_new, y_new, w_new, h_new))\n",
        "                    smoothed_bbox = moving_average(list(bbox_history))\n",
        "\n",
        "                    last_speaker_bbox = smoothed_bbox\n",
        "                else:\n",
        "\n",
        "                    stacked_frames = []\n",
        "                    for x, y, w, h in last_detected_faces[:2]:\n",
        "                        expand_factor = 3.2\n",
        "                        x_new = max(0, int(x - (w * (expand_factor - 1) / 2)))\n",
        "                        w_new = min(width - x_new, int(w * expand_factor))\n",
        "                        y_new = max(0, int(y - (h * (expand_factor - 1) / 2)))\n",
        "                        h_new = min(height - y_new, int(h * expand_factor))\n",
        "\n",
        "                        bbox_history.append((x_new, y_new, w_new, h_new))\n",
        "                        smoothed_bbox = moving_average(list(bbox_history))\n",
        "\n",
        "                        x_s, y_s, w_s, h_s = smoothed_bbox\n",
        "                        person_roi = frame[y_s:y_s + h_s, x_s:x_s + w_s]\n",
        "                        resized_person = cv2.resize(person_roi, (output_width, output_height // 2))\n",
        "                        stacked_frames.append(resized_person)\n",
        "\n",
        "                    if len(stacked_frames) == 2:\n",
        "                        padded_frame = np.vstack(stacked_frames)\n",
        "                    else:\n",
        "                        padded_frame = stacked_frames[0]\n",
        "\n",
        "                    out.write(padded_frame)\n",
        "                    frame_number += 1\n",
        "                    continue\n",
        "        else:\n",
        "            smoothed_bbox = last_speaker_bbox\n",
        "\n",
        "        if smoothed_bbox:\n",
        "            x_s, y_s, w_s, h_s = smoothed_bbox\n",
        "            person_roi = frame[y_s:y_s + h_s, x_s:x_s + w_s]\n",
        "\n",
        "            aspect_ratio = w_s / h_s\n",
        "            target_aspect_ratio = output_width / output_height\n",
        "\n",
        "            if aspect_ratio > target_aspect_ratio:\n",
        "                new_width = output_width\n",
        "                new_height = int(output_width / aspect_ratio)\n",
        "            else:\n",
        "                new_height = output_height\n",
        "                new_width = int(output_height * aspect_ratio)\n",
        "\n",
        "            resized = cv2.resize(person_roi, (new_width, new_height))\n",
        "\n",
        "\n",
        "            padded_frame = np.zeros((output_height, output_width, 3), dtype=np.uint8)\n",
        "            start_x = (output_width - new_width) // 2\n",
        "            start_y = (output_height - new_height) // 2\n",
        "            padded_frame[start_y:start_y + new_height, start_x:start_x + new_width] = resized\n",
        "        else:\n",
        "\n",
        "            resized = cv2.resize(frame, (output_width, int(output_width * height / width)))\n",
        "            padded_frame = np.zeros((output_height, output_width, 3), dtype=np.uint8)\n",
        "            start_y = (output_height - resized.shape[0]) // 2\n",
        "            padded_frame[start_y:start_y + resized.shape[0], :, :] = resized\n",
        "\n",
        "        out.write(padded_frame)\n",
        "        frame_number += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "\n",
        "\n",
        "def merge_audio(video_path, audio_clip, output_path):\n",
        "    video = VideoFileClip(video_path)\n",
        "    video = video.set_audio(audio_clip)\n",
        "    video.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n",
        "\n",
        "input_video = \"output.mp4\"\n",
        "output_video = \"face_tracked_output.mp4\"\n",
        "\n",
        "audio_file, audio_clip = extract_audio(input_video)\n",
        "speech_times = detect_speech(audio_file)\n",
        "\n",
        "process_video(input_video, output_video, speech_times, detection_skip=5, smoothing_window=10)\n",
        "\n",
        "merge_audio(\"temp_video.mp4\", audio_clip, output_video)\n",
        "os.remove(\"temp_video.mp4\")\n",
        "print(\"Video processing complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN6HjW8E7mrN"
      },
      "source": [
        "##adding sub titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-zXOxiuF6SY",
        "outputId": "9567a538-6882-4360-d619-a5e11d47047c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/800.5 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.61.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2.0.0->openai-whisper) (3.17.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.44.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803373 sha256=6d1c08b9ec3c08e26d7e8b2ecc9b8c9c9ba47f694384cc2b64d9062bbd87a8de\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1eFgsFiGAi5",
        "outputId": "ddb32373-7b7d-4bc9-d6d1-9e9cd01e9177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-71WsrsKjAN",
        "outputId": "189c2398-3092-4776-d52c-18f89d21b09a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:69: UserWarning: /root/.cache/whisper/large-v3.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
            "  warnings.warn(\n",
            "\n",
            "100%|█████████████████████████████████████| 2.88G/2.88G [00:38<00:00, 80.5MiB/s]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Building video subtitled_output.mp4.\n",
            "MoviePy - Writing audio in subtitled_outputTEMP_MPY_wvf_snd.mp3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video subtitled_output.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready subtitled_output.mp4\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "import cv2\n",
        "import moviepy.editor as mp\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def transcribe_with_word_timestamps(audio_path):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = whisper.load_model(\"large\").to(device)\n",
        "\n",
        "\n",
        "    result = model.transcribe(audio_path, word_timestamps=True)\n",
        "\n",
        "\n",
        "    word_segments = []\n",
        "    for segment in result[\"segments\"]:\n",
        "        for word in segment[\"words\"]:\n",
        "            word_segments.append((word[\"start\"], word[\"end\"], word[\"word\"]))\n",
        "\n",
        "    return word_segments\n",
        "\n",
        "def overlay_live_subtitles(video_path, subtitles, output_path):\n",
        "    video = mp.VideoFileClip(video_path)\n",
        "\n",
        "    def add_text(get_frame, t):\n",
        "        frame = get_frame(t)\n",
        "        h, w, _ = frame.shape\n",
        "        overlay = frame.copy()\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        font_scale = 1.5\n",
        "        font_thickness = 3\n",
        "\n",
        "        current_text = \" \".join([word for start, end, word in subtitles if start <= t <= end])\n",
        "\n",
        "        if current_text:\n",
        "            text_size = cv2.getTextSize(current_text, font, font_scale, font_thickness)[0]\n",
        "            text_x = (w - text_size[0]) // 2\n",
        "            text_y = h - 200\n",
        "\n",
        "            padding = 20\n",
        "            bg_x1, bg_y1 = text_x - padding, text_y - text_size[1] - padding\n",
        "            bg_x2, bg_y2 = text_x + text_size[0] + padding, text_y + padding\n",
        "            cv2.rectangle(overlay, (bg_x1, bg_y1), (bg_x2, bg_y2), (0, 0, 0), -1)\n",
        "\n",
        "            alpha = 0.5\n",
        "            frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
        "\n",
        "            cv2.putText(frame, current_text, (text_x, text_y), font, font_scale, (255, 255, 255), font_thickness, cv2.LINE_AA)\n",
        "\n",
        "        return frame\n",
        "\n",
        "\n",
        "    new_video = video.fl(add_text)\n",
        "    new_video.write_videofile(output_path, fps=video.fps)\n",
        "\n",
        "word_subtitles = transcribe_with_word_timestamps(\"face_tracked_output.mp4\")\n",
        "overlay_live_subtitles(\"face_tracked_output.mp4\", word_subtitles, \"subtitled_output.mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "030mRbJtMp5a",
        "outputId": "30bc115a-5237-44e7-df52-b44811df92b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/121.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoL2GQomLxjH",
        "outputId": "c62b67c2-797e-4d48-f19a-c98154bc5ae8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " and difficult and has it got better or is it something you're always working on so on one side of things struggle that school as i said so the teachers would write these reports that i was not focused or i wasn't doing well and i remember the fear of that report every year and i try so hard to do well and then these teachers i don't know if teachers out there realize when they write those reports what's happening back home whether you have an abusive household you know the the stress of that was was difficult i feel in racing if i if i would win i could see a smile on my that's fair and it was really like okay if I do well at this I know that I'll be accepted um but I've got to work double hard to be I've got to always be first I always laugh about the whole if you're not first you're last because I'm literally say I've not been first my whole life first was everything yeah um in order to be accepted in order to and maybe to be appreciated um not only in within my relationship perhaps with my dad but then also my friends and it wasn't until I got older I realized it's about the bigger picture but when you have success it's so short-lived it is really short-lived you win a race and you go home and you have big as a race you know the weekend is so intense you've seen there's so much energy so much in a stressful environment for everyone that's working within it and you go home and there's a huge come down one or two days later and you're trying to balance those emotions that emotional roller coaster um and learning to kind of channel that and figure out ways to keep it balanced whether with your team and those sorts of things has been really key but i think during the last couple last few really understanding that it's about the bigger picture um i'm fighting for something far greater than winning a race. I'm really finding\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key='YOUR_API_KEY')\n",
        "filename = 'subtitled_output.mp4'\n",
        "\n",
        "with open(filename, \"rb\") as file:\n",
        "    transcription = client.audio.transcriptions.create(\n",
        "      file=(filename, file.read()),\n",
        "      model=\"whisper-large-v3\",\n",
        "      response_format=\"verbose_json\",\n",
        "    )\n",
        "    print(transcription.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jcNMZQzSN3tM"
      },
      "outputs": [],
      "source": [
        "transcription_text = transcription.text\n",
        "transcription_segments = transcription.segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wZYF6zEyLtKZ"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "import json\n",
        "\n",
        "def extract_keywords(transcribed_text):\n",
        "    client = Groq(api_key='YOUR_API_KEY')\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"you are a image's keyword provider assistent, you whole task is to look into the transcribed text and by understanding the transcribed text you need to look for the important keywords or phrases and give me the 'image_keyword' in JSON such that i can use those keyword to search the web and find images of that. and those keywords should be or phrases should be in the transcribed text.\"\n",
        "        },\n",
        "\n",
        "       {\"role\": \"assistant\",\n",
        "            \"content\": transcribed_text}\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_completion_tokens=1024,\n",
        "    top_p=1,\n",
        "    stream=False,\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    stop=None,\n",
        "    )\n",
        "\n",
        "    response_text = completion.choices[0].message.content\n",
        "    keyword_data = json.loads(response_text)\n",
        "    return keyword_data.get(\"image_keyword\", [])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5ftc7tvCaWq",
        "outputId": "1af1b753-06ad-4c93-fdea-15d0e351588e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bing-image-downloader\n",
            "  Downloading bing_image_downloader-1.1.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Downloading bing_image_downloader-1.1.2-py3-none-any.whl (5.9 kB)\n",
            "Installing collected packages: bing-image-downloader\n",
            "Successfully installed bing-image-downloader-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install bing-image-downloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IlRWC2jnNPe1"
      },
      "outputs": [],
      "source": [
        "from bing_image_downloader import downloader\n",
        "\n",
        "def download_images(keywords, limit=1):\n",
        "    image_paths = {}\n",
        "\n",
        "    for keyword in keywords:\n",
        "        downloader.download(keyword, limit=limit, output_dir=\"images\", adult_filter_off=True, force_replace=False, timeout=60)\n",
        "        image_paths[keyword] = f\"images/{keyword}\"\n",
        "\n",
        "    return image_paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1KIIiKdMyfU",
        "outputId": "59d202d5-c42a-423a-8957-b126f5a8bf3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[%] Downloading Images to /content/images/racing\n",
            "\n",
            "\n",
            "[!!]Indexing page: 1\n",
            "\n",
            "[%] Indexed 1 Images on Page 1.\n",
            "\n",
            "===============================================\n",
            "\n",
            "[%] Downloading Image #1 from https://images.pexels.com/photos/158971/pexels-photo-158971.jpeg?cs=srgb&amp;dl=auto-racing-car-championship-158971.jpg&amp;fm=jpg\n",
            "[%] File Downloaded !\n",
            "\n",
            "\n",
            "\n",
            "[%] Done. Downloaded 1 images.\n",
            "[%] Downloading Images to /content/images/school reports\n",
            "\n",
            "\n",
            "[!!]Indexing page: 1\n",
            "\n",
            "[%] Indexed 1 Images on Page 1.\n",
            "\n",
            "===============================================\n",
            "\n",
            "[%] Downloading Image #1 from http://www.samplestemplates.org/wp-content/uploads/2015/05/school-Report-Template-image-.png\n",
            "[%] File Downloaded !\n",
            "\n",
            "\n",
            "\n",
            "[%] Done. Downloaded 1 images.\n",
            "[%] Downloading Images to /content/images/abusive household\n",
            "\n",
            "\n",
            "[!!]Indexing page: 1\n",
            "\n",
            "[%] Indexed 1 Images on Page 1.\n",
            "\n",
            "===============================================\n",
            "\n",
            "[%] Downloading Image #1 from https://domestic-violence.laws.com/wp-content/uploads/sites/79/2019/12/4dbb42db00924.jpg\n",
            "[%] File Downloaded !\n",
            "\n",
            "\n",
            "\n",
            "[%] Done. Downloaded 1 images.\n",
            "[%] Downloading Images to /content/images/stress\n",
            "\n",
            "\n",
            "[!!]Indexing page: 1\n",
            "\n",
            "[%] Indexed 1 Images on Page 1.\n",
            "\n",
            "===============================================\n",
            "\n",
            "[%] Downloading Image #1 from https://blog.tlsslim.com/wp-content/uploads/2020/06/4F320E28-D154-4DF8-9A31-A55ACB5CB64D.jpeg\n",
            "[%] File Downloaded !\n",
            "\n",
            "\n",
            "\n",
            "[%] Done. Downloaded 1 images.\n",
            "[%] Downloading Images to /content/images/winning a race\n",
            "\n",
            "\n",
            "[!!]Indexing page: 1\n",
            "\n",
            "[%] Indexed 1 Images on Page 1.\n",
            "\n",
            "===============================================\n",
            "\n",
            "[%] Downloading Image #1 from https://thumbs.dreamstime.com/z/happy-young-male-runner-winning-race-finish-fitness-sport-victory-success-healthy-lifestyle-concept-men-coming-first-to-60474208.jpg\n",
            "[%] File Downloaded !\n",
            "\n",
            "\n",
            "\n",
            "[%] Done. Downloaded 1 images.\n",
            "[%] Downloading Images to /content/images/emotional roller coaster\n",
            "\n",
            "\n",
            "[!!]Indexing page: 1\n",
            "\n",
            "[%] Indexed 1 Images on Page 1.\n",
            "\n",
            "===============================================\n",
            "\n",
            "[%] Downloading Image #1 from https://i.pinimg.com/originals/3c/8e/2a/3c8e2a431074549f5565503b3a3a95df.jpg\n",
            "[%] File Downloaded !\n",
            "\n",
            "\n",
            "\n",
            "[%] Done. Downloaded 1 images.\n",
            "[%] Downloading Images to /content/images/bigger picture\n",
            "\n",
            "\n",
            "[!!]Indexing page: 1\n",
            "\n",
            "[%] Indexed 1 Images on Page 1.\n",
            "\n",
            "===============================================\n",
            "\n",
            "[%] Downloading Image #1 from http://elitetrack.com/wp-content/uploads/2014/10/bigstock-The-Big-Picture-Concept-36887548.jpg\n",
            "[!] Issue getting: http://elitetrack.com/wp-content/uploads/2014/10/bigstock-The-Big-Picture-Concept-36887548.jpg\n",
            "[!] Error:: Remote end closed connection without response\n",
            "\n",
            "\n",
            "[!!]Indexing page: 2\n",
            "\n",
            "[%] Indexed 35 Images on Page 2.\n",
            "\n",
            "===============================================\n",
            "\n",
            "[%] Downloading Image #1 from https://theconsciousvibe.com/wp-content/uploads/2021/10/how-to-see-seeing-the-bigger-big-picture-perspective-earth-view-the-conscious-vibe-life-.webp\n",
            "[%] File Downloaded !\n",
            "\n",
            "\n",
            "\n",
            "[%] Done. Downloaded 1 images.\n",
            "{'racing': 'images/racing', 'school reports': 'images/school reports', 'abusive household': 'images/abusive household', 'stress': 'images/stress', 'winning a race': 'images/winning a race', 'emotional roller coaster': 'images/emotional roller coaster', 'bigger picture': 'images/bigger picture'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "keywords = extract_keywords(transcription_text)\n",
        "image_paths = download_images(keywords)\n",
        "\n",
        "print(image_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TJQ0FhtxQvyK"
      },
      "outputs": [],
      "source": [
        "def get_keyword_timestamps(transcription_segments, keywords):\n",
        "    keyword_timestamps = {}\n",
        "\n",
        "    for segment in transcription_segments:\n",
        "        text = segment[\"text\"]\n",
        "        start = segment[\"start\"]\n",
        "        end = segment[\"end\"]\n",
        "\n",
        "        for keyword in keywords:\n",
        "            if keyword.lower() in text.lower():\n",
        "                if keyword not in keyword_timestamps:\n",
        "                    keyword_timestamps[keyword] = []\n",
        "                keyword_timestamps[keyword].append((max(0, start - 0.5), end + 0.5))\n",
        "\n",
        "    return keyword_timestamps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FsSzAet4SwQG"
      },
      "outputs": [],
      "source": [
        "from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip\n",
        "import os\n",
        "\n",
        "def get_first_image_path(folder):\n",
        "    if os.path.exists(folder):\n",
        "        files = [f for f in os.listdir(folder) if f.endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
        "        return os.path.join(folder, files[0]) if files else None\n",
        "    return None\n",
        "\n",
        "def add_images_to_video(video_path, keyword_timestamps, image_paths, output_path):\n",
        "    video = VideoFileClip(video_path)\n",
        "    video_w, video_h = video.size\n",
        "\n",
        "    overlays = []\n",
        "\n",
        "    for keyword, timestamps in keyword_timestamps.items():\n",
        "        img_path = get_first_image_path(image_paths[keyword])\n",
        "\n",
        "        if not img_path:\n",
        "            print(f\"No valid image found for {keyword}\")\n",
        "            continue\n",
        "\n",
        "        img_clip = (ImageClip(img_path, transparent=True)\n",
        "                    .set_duration(1.0)\n",
        "                    .resize(width=video_w * 0.8)\n",
        "                    .set_opacity(1))\n",
        "\n",
        "        for start, end in timestamps:\n",
        "            fade_duration = 0.2\n",
        "            print(f\"Overlaying {keyword} from {start}s to {end}s\")\n",
        "\n",
        "\n",
        "            clip = (img_clip\n",
        "                    .set_position((\"center\", \"top\"))\n",
        "                    .set_start(start)\n",
        "                    .set_duration(end - start)\n",
        "                    .crossfadein(fade_duration)\n",
        "                    .crossfadeout(fade_duration))\n",
        "\n",
        "            overlays.append(clip)\n",
        "\n",
        "    if not overlays:\n",
        "        print(\"No overlays were added. Check timestamps and image paths.\")\n",
        "\n",
        "\n",
        "    final_video = CompositeVideoClip([video] + overlays, size=(video_w, video_h))\n",
        "    final_video.write_videofile(output_path, fps=video.fps, codec=\"libx264\", threads=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR0_rd_QS6Ve",
        "outputId": "52b140c5-11d7-4c9c-8032-8219adeae1cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Overlaying abusive household from 17.04s to 22.4s\n",
            "✅ Overlaying racing from 21.4s to 28.24s\n",
            "✅ Overlaying stress from 21.4s to 28.24s\n",
            "✅ Overlaying stress from 61.04s to 66.08s\n",
            "⚠️ No valid image found for bigger picture\n",
            "✅ Overlaying emotional roller coaster from 68.48s to 74.82s\n",
            "✅ Overlaying winning a race from 86.16s to 88.52s\n",
            "Moviepy - Building video final_output.mp4.\n",
            "MoviePy - Writing audio in final_outputTEMP_MPY_wvf_snd.mp3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video final_output.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "t: 100%|██████████| 2127/2127 [01:55<00:00, 29.71it/s, now=None]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file subtitled_output.mp4, 2764800 bytes wanted but 0 bytes read,at frame 2126/2127, at time 88.62/88.63 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready final_output.mp4\n"
          ]
        }
      ],
      "source": [
        "keyword_timestamps = get_keyword_timestamps(transcription_segments, keywords)\n",
        "\n",
        "add_images_to_video(\"subtitled_output.mp4\", keyword_timestamps, image_paths, \"final_output.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML55N51oTjCT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
